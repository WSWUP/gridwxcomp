import datetime
import ee
import os
import pandas as pd
import re

from google.cloud import storage
from pathlib import Path


end_date = '2023-01-01'

DATASET_METADATA = {
    "conus404": {
        "path": "projects/openet/meteorology/conus/conus404/daily",
        "start_date": "1979-10-01",
        "end_date": end_date},
    "era5land": {
        "path": "projects/openet/assets/meteorology/era5land/na/daily",
        "start_date": "1980-01-01",
        "end_date": end_date},
    "gridmet": {
          "path": "IDAHO_EPSCOR/GRIDMET",
          "start_date": "1979-01-01",
          "end_date": "2023-10-26"},
    "openet_gridmet": {
        "path": "projects/openet/reference_et/conus/gridmet/daily/v1",
        "start_date": "1980-01-01",
        "end_date": end_date},
    "nldas": {  # NLDAS is an hourly dataset
          "path": "NASA/NLDAS/FORA0125_H002",
          "start_date": "1979-01-01",
          "end_date": end_date},
    "noaa_nldas": {
          "path": "projects/eddi-noaa/nldas/daily",
          "start_date": "1979-01-02",
          "end_date": end_date},
    "rtma": {  # RTMA is an hourly dataset
          "path": "NOAA/NWS/RTMA",
          "start_date": "2011-01-01",
          "end_date": end_date},
    "ce_rtma": {
        "path": "projects/climate-engine/rtma/daily",
        "start_date": "2015-07-01",
        "end_date": end_date},
    "spatial_cimis": {
        "path": "projects/climate-engine/cimis/daily",
        "start_date": "2003-10-01",
        "end_date": end_date}
}


def _download_point_data(start_date, end_date, lat, lon, station_name, dataset_path, dataset_name, bucket, path):

    start_name = start_date.replace('-', '')
    end_name = (datetime.datetime.strptime(end_date, "%Y-%m-%d") - datetime.timedelta(days=1)).strftime('%Y%m%d')

    # Create point to reduce over
    point = ee.Geometry.Point([lon, lat])

    ic = ee.ImageCollection(dataset_path).filterDate(start_date, end_date)

    station_string = station_name
    station_name = ee.String(station_name)
    file_description = '{}_{}_{}_{}'.format(dataset_name, station_string, start_name, end_name)
    complete_path = path + file_description + '_all_vars'

    def _reduce_point_img(img):
        date_str = img.date()
        date_mean = date_str.format("YYYYMMdd")
        # TODO add nominal scale call and set reducing scale to it
        reduce_mean = img.reduceRegion(geometry=point,
                                       reducer=ee.Reducer.mean(),
                                       scale=1000)

        return ee.Feature(None, reduce_mean).set({"date": date_mean, 'station_name': station_name})

    output = ee.FeatureCollection(ic.map(_reduce_point_img))

    # Export Summary Table
    task = ee.batch.Export.table.toCloudStorage(
        collection=ee.FeatureCollection(output),
        description=file_description,
        bucket=bucket, fileNamePrefix=complete_path, fileFormat='CSV',
    )

    task.start()

    print('Request submitted for {}_{}_{}_{}'.format(dataset_name, station_string, start_name, end_name))
    print(f'Waiting for task (id: {task.id}) to complete ...')
    while task.active():
        continue


def download_grid_data(metadata_path, dataset, export_bucket, export_path, force_download=False, authorize=True):
    """
        Takes in the metadata file generated by "prep_metadata" and downloads the
        corresponding point data for all stations within. This function requires the dataset be
        present in the DATASET_METADATA dictionary

        The metadata file will be updated for the path the files will be in the bucket,
        but they must be manually downloaded

        Arguments:
            metadata_path (str): path to the metadata path generated by "prep_metadata"
            dataset (str): name of the dataset that
            export_bucket (str): name of the Google cloud bucket for export
            export_path (str): path within bucket where data is going to be saved
            force_download (bool): will re-download all data even if bucket filepath already exists
            authorize (bool): if true with authenticate with ee before initializing

        Returns:
            None

    """

    # Authenticate and initialize with earth engine
    if authorize:
        ee.Authenticate()
    else:
        pass
    ee.Initialize(project='openet-dri')

    # Initialize local directory files will be downloaded to
    Path(f'{dataset}/').mkdir(parents=True, exist_ok=True)

    # Pull relevant metadata from dictionary
    gridded_dataset_date_start = DATASET_METADATA[dataset]['start_date']
    gridded_dataset_date_end = DATASET_METADATA[dataset]['end_date']
    gridded_dataset_end_name = (datetime.datetime.strptime(gridded_dataset_date_end, "%Y-%m-%d") -
                                datetime.timedelta(days=1)).strftime('%Y%m%d')
    gridded_dataset_path = DATASET_METADATA[dataset]['path']

    # Open gridwxcomp station metadata file
    metadata_df = pd.read_csv(metadata_path)
    metadata_df['GRID_FILE_PATH'] = ''

    # Connect to google bucket to check which files have been downloaded
    storage_client = storage.Client()
    storage_bucket = storage_client.bucket(export_bucket)

    # Iterate over metadata_df
    for index, row in metadata_df.iterrows():
        print(f'Extracting {dataset} data for: {row["STATION_ID"]}')
        formatted_station_id = re.sub(r'\W+', '', row["STATION_ID"].replace(' ', '_')).lower()
        print(f'Formatted Station_ID: {formatted_station_id}')

        local_path = f'{dataset}/{dataset}_{formatted_station_id}_' \
            f'{gridded_dataset_date_start.replace("-", "")}_{gridded_dataset_end_name}_all_vars.csv'
        cloud_path = f'{export_path}{dataset}_{formatted_station_id}_' \
            f'{gridded_dataset_date_start.replace("-", "")}_{gridded_dataset_end_name}_all_vars.csv'

        metadata_df.loc[index, 'GRID_FILE_PATH'] = local_path

        # Check if file exists on the cloud and skip unless force_download is true
        gcloud_blob = storage.Blob(bucket=storage_bucket, name=cloud_path)
        cloud_file_exists = gcloud_blob.exists(storage_client)
        if cloud_file_exists and not force_download:
            print(f'gs://{export_bucket}/{cloud_path} already exists, skipping.')
        else:
            _download_point_data(start_date=gridded_dataset_date_start, end_date=gridded_dataset_date_end,
                                 lat=row['STATION_LAT'], lon=row['STATION_LON'],
                                 station_name=str(formatted_station_id),
                                 dataset_path=gridded_dataset_path, dataset_name=dataset,
                                 bucket=export_bucket, path=export_path)
        if not os.path.exists(local_path):
            print('Downloading', local_path, '...')
            gcloud_blob.download_to_filename(local_path)

    print('All points have been requested and the updated metadata has been saved.')
    metadata_df.to_csv(metadata_path)


if __name__ == '__main__':
    print('\n--------------------------------------------------------'
          ' Functionality for running this library from the terminal'
          ' was removed. Please refer to the documentation on how to'
          ' make calls to these functions. \n\n')
