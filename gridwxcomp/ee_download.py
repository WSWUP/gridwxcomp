import datetime
import ee
import os
import pandas as pd
import re

from google.cloud import storage
from pathlib import Path
from gridwxcomp.util import read_config

def _get_collection_date_range(path):
    """
        Acquires date range for EE image collection if dates are not provided
        by the user in the config file

        Arguments:
            path (str): path to image collection on earth engine

        Returns:
            start_date (str): date of first image as YYYY-MM-DD
            end_date (str): date of last image as YYYY-MM-DD
    """
    dataset = ee.ImageCollection(path)
    start_img = dataset.limit(1, 'system:index', True).first()
    start_text = start_img.getInfo()['properties']['system:index']
    start_date = start_text[:4] + '-' + start_text[4:]
    start_date = start_date[:7] + '-' + start_date[7:]
    end_img = dataset.limit(1, 'system:index', False).first()
    end_text = end_img.getInfo()['properties']['system:index']
    end_date = end_text[:4] + '-' + end_text[4:]
    end_date = end_date[:7] + '-' + end_date[7:]
    return start_date, end_date

def _download_point_data(
        start_date,
        end_date,
        lat,
        lon,
        station_name,
        dataset_path,
        dataset_name,
        bucket,
        path):

    start_name = start_date.replace('-', '')
    end_name = (
        datetime.datetime.strptime(
            end_date,
            "%Y-%m-%d") -
        datetime.timedelta(
            days=1)).strftime('%Y%m%d')

    # Create point to reduce over
    point = ee.Geometry.Point([lon, lat])

    ic = ee.ImageCollection(dataset_path).filterDate(start_date, end_date)

    station_string = station_name
    station_name = ee.String(station_name)
    file_description = '{}_{}_{}_{}'.format(
        dataset_name, station_string, start_name, end_name)
    complete_path = path + file_description + '_all_vars'

    def _reduce_point_img(img):
        date_str = img.date()
        date_mean = date_str.format("YYYYMMdd")
        # TODO add nominal scale call and set reducing scale to it
        reduce_mean = img.reduceRegion(geometry=point,
                                       reducer=ee.Reducer.mean(),
                                       crs='EPSG:4326',
                                       scale=1000)

        return ee.Feature(None, reduce_mean).set(
            {"date": date_mean, 'station_name': station_name})

    output = ee.FeatureCollection(ic.map(_reduce_point_img))

    # Export Summary Table
    task = ee.batch.Export.table.toCloudStorage(
        collection=ee.FeatureCollection(output),
        description=file_description,
        bucket=bucket, fileNamePrefix=complete_path, fileFormat='CSV',
    )

    task.start()

    print(
        'Request submitted for {}_{}_{}_{}'.format(
            dataset_name,
            station_string,
            start_name,
            end_name))
    print(f'Waiting for task (id: {task.id}) to complete ...')
    while task.active():
        continue


def download_grid_data(
        metadata_path,
        config_path,
        export_bucket,
        export_path,
        local_folder=None,
        force_download=False):
    """
        Takes in the metadata file generated by "prep_metadata" and downloads
        the corresponding point data for all stations within. This function
        requires the dataset be present in the DATASET_METADATA dictionary

        The metadata file will be updated for the path the gridded data files 
        are downloaded to.

        Arguments:
            metadata_path (str): path to the metadata path generated by "prep_metadata"
            config_path (str): path to config file containing catalog info
            export_bucket (str): name of the Google cloud bucket for export
            export_path (str): path within bucket where data is going to be saved
            local_folder (str): folder to download point data to
            force_download (bool): will re-download all data even if bucket filepath already exists

        Returns:
            None

        Note: You must authenticate with Google Earth Engine before using
            this function.

    """

    config = read_config(config_path)  # Read config
    # Pull relevant metadata from dictionary
    dataset = config['collection_info']['name']
    gridded_dataset_path = config['collection_info']['path']
    gridded_dataset_date_start = config['collection_info']['start_date']
    gridded_dataset_date_end = config['collection_info']['end_date']

    # Fill in start/end dates if either are missing
    collection_start_date, collection_end_date = (
        _get_collection_date_range(gridded_dataset_path))
    if gridded_dataset_date_start == '':
        gridded_dataset_date_start = collection_start_date
    if gridded_dataset_date_end == '':
        gridded_dataset_date_end = collection_end_date

    gridded_dataset_end_name = (
        datetime.datetime.strptime(
            gridded_dataset_date_end,
            "%Y-%m-%d") -
        datetime.timedelta(
            days=1)).strftime('%Y%m%d')

    # Open gridwxcomp station metadata file
    metadata_df = pd.read_csv(metadata_path)
    metadata_df['GRID_FILE_PATH'] = ''

    # Connect to google bucket to check which files have been downloaded
    storage_client = storage.Client()
    storage_bucket = storage_client.bucket(export_bucket)

    # Iterate over metadata_df
    for index, row in metadata_df.iterrows():
        print(f'Extracting {dataset} data for: {row["STATION_ID"]}')
        formatted_station_id = re.sub(
            r'\W+', '',
            row["STATION_ID"].replace(' ', '_')).lower()
        print(f'Formatted Station_ID: {formatted_station_id}')

        if local_folder:
            Path(f'{local_folder}/{dataset}').mkdir(parents=True, exist_ok=True)
            local_path = (f'{local_folder}/{dataset}/{dataset}_'
                          f'{formatted_station_id}_'
                          f'{gridded_dataset_date_start.replace("-", "")}_'
                          f'{gridded_dataset_end_name}_all_vars.csv')
        else:
            Path(f'{dataset}').mkdir(parents=True, exist_ok=True)
            local_path = f'{dataset}/{dataset}_{formatted_station_id}_' \
                f'{gridded_dataset_date_start.replace("-", "")}_{gridded_dataset_end_name}_all_vars.csv'

        cloud_path = f'{export_path}{dataset}_{formatted_station_id}_' \
            f'{gridded_dataset_date_start.replace("-", "")}_{gridded_dataset_end_name}_all_vars.csv'

        metadata_df.loc[index, 'GRID_FILE_PATH'] = local_path

        # Check if file exists on the cloud and skip unless force_download is
        # true
        gcloud_blob = storage.Blob(bucket=storage_bucket, name=cloud_path)
        cloud_file_exists = gcloud_blob.exists(storage_client)
        if cloud_file_exists and not force_download:
            print(
                f'gs://{export_bucket}/{cloud_path} already exists, skipping.')
        else:
            _download_point_data(
                start_date=gridded_dataset_date_start,
                end_date=gridded_dataset_date_end,
                lat=row['STATION_LAT_WGS84'],
                lon=row['STATION_LON_WGS84'],
                station_name=str(formatted_station_id),
                dataset_path=gridded_dataset_path,
                dataset_name=dataset,
                bucket=export_bucket,
                path=export_path)
        if not os.path.exists(local_path):
            print('Downloading', local_path, '...')
            gcloud_blob.download_to_filename(local_path)

    print('All points have been requested and the updated metadata has been saved.')
    metadata_df.to_csv(metadata_path)


if __name__ == '__main__':
    print('\n--------------------------------------------------------'
          ' Functionality for running this library from the terminal'
          ' was removed. Please refer to the documentation on how to'
          ' make calls to these functions. \n\n')
